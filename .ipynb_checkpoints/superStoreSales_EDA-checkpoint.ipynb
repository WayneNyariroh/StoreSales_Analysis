{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87725b5c-eb5b-4212-8b30-1f248f3ac772",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis using Python\n",
    "#### by ***Wayne Willis Omondi***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2622c318-cb47-4e57-916f-93c43d4ce0ad",
   "metadata": {},
   "source": [
    "## 1.0: Introduction\n",
    "\n",
    "In this modern times where every business is highly dependent on its data *(Data is King)* to make better decisions for developing business, data analysis plays an important role in helping different business entities to get an idea on their performance and any opportunities to increase gains and minimise losses. Objective is to gain valuable insights on the overall performance of the store.\n",
    "\n",
    "For our analysis we will be using the [SuperStore dataset](https://community.tableau.com/s/question/0D54T00000CWeX8SAL/sample-superstore-sales-excelxls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e98fcf-4aca-4604-93f1-c435221bf7e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1: Why Python?\n",
    "\n",
    "**Python** is a popular choice for Data Analysis due to the many helpful analytics libraries and even for scientific computing, machine learning and more complex tasks. Combined with Python's overall strength for general-purpose software engineering, it is an excellent tool for building data applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba1282-812c-4bae-aa8d-68df219f1744",
   "metadata": {},
   "source": [
    "### 1.2: Our Tools\n",
    "For our EDA, we will be using the following libraries:\n",
    "-  **pandas** (a library that makes working with structured and tabular data fast, easy and expressive).\n",
    "-  **numpy** (a library that provides the data structures and algorithms useful for numerical computing).\n",
    "-  **matplotlip** (library for plots and two-dimensional visualizations).\n",
    "-  **seaborn** (statistical data visualization library).\n",
    "\n",
    "Let's not forget *Jupyter* library that allows as to present our code in form of an interactive notebook/document with text, plots and other outputs (even a terminal - through magic commands)\n",
    "All these have already been install in our python virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd5ec6-9944-4513-a366-f36c3c9a0cf2",
   "metadata": {},
   "source": [
    "### 1.3: Importing Our Libraries and Some Necessary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497742d-32a0-46e5-a6af-3db208d0ff3e",
   "metadata": {},
   "source": [
    "Since we are using the pandas and numpy libraries for our data processing and manipulation and the matplotlib and seaborn libraries for data vizualization, to start off we have to import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b7fea-944a-4dc0-a4d2-3a88dde4ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4737c-d343-4f7e-ba38-53a9dcacb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4448ee6-17a5-4d3f-b875-d0771c70dc99",
   "metadata": {},
   "source": [
    "Additional this being a Sales dataset we will likely be working with dates and/or time. So inorder to be able to manipulate those we will make an additional import of the 'datetime' function in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b3d46-891c-44c3-88f4-2b331ccb7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aff5f3-6abb-43e2-97be-3befbeb16328",
   "metadata": {},
   "source": [
    "We will also be disabling the warnings in the jupyter, setting the filter to never display warnings.\n",
    "[More on Warnings](https://www.geeksforgeeks.org/warnings-in-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c13555-f6c1-4aac-bc85-332b2cad0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7907644-4899-4c02-811f-c5fa4e4ffc27",
   "metadata": {},
   "source": [
    "## 2.0: Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10139d-40d9-461c-8536-14203e087add",
   "metadata": {},
   "source": [
    "### 2.1: Loading Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c2e2f-61dd-4cbc-93f5-d415c3353c11",
   "metadata": {},
   "source": [
    "In our case our data is stored in a csv format so we will use the .read_csv() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb20d9d-9c14-4c7f-984d-7fda69ae29f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df = pd.read_csv('data/SuperStoreSales_Whole.csv') #loading our dataset into a dataframe named store_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7633fa8-d799-43f2-8279-895d58be6eff",
   "metadata": {},
   "source": [
    "### 2.2: Viewing Our Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf66b30-356b-4ad5-9936-e534538994d8",
   "metadata": {},
   "source": [
    "We have to see what we are working with and whether any data cleaning is necessary. In this step we display our dataset, get to know how many columns and rows are present, how much data is missing, present datatypes in each column, unique features and last but least a statistical description of our dataset.\n",
    "While doing this we will also get to understand more about the store, for example, what products they sells, who they sell to and where, before we dive into their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16557d-9cc9-49ba-8efb-0c4701994c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 6) #display 6 rows of our dataset, 3 head, 3 tail - commented out for now\n",
    "store_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7654f07-0d17-490e-a7db-3f59392ac77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.shape #view the shape of our dataset - total rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3202889-6000-4d25-8935-88c65e0d399d",
   "metadata": {},
   "source": [
    "Our dataset has 9800 rows and 21 columns. Depending on our objective with our EDA we could drop some columns from the analysis, for example the *'Customer Name'* columns is not necessary. In certain case personal information is often excluded from a data analysis, unless in our case if the store intends to award the most loyal and/or top customer. In that case we could still drop the *'Customer Name'* column and retain the *'Customer ID'* column for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e10508-2490-4bd8-adcd-9ee4ef38e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.drop(columns = ['Customer Name', 'Row ID'], inplace=True) #remove 'Customer Name' and 'Row ID' columns from the 'store_df' dataframe\n",
    "store_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806bbd7-4750-4e5f-8f44-782cd494048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.columns #view all the columns, incase we need to rename any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71715234-42d5-45de-bbc1-814a23d2c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.info() #view information on our dataframe interms of index range and datatype for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db788d3-399b-4772-803f-888c4e5f9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.describe() #statistical description of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f11899-8bd2-48b6-b8a3-69bb4a4361d4",
   "metadata": {},
   "source": [
    "#### 2.2.1: Check for any Duplicated Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d3645-dca3-428e-8795-2aa7143f48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df[store_df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679c4e5-c740-4d40-a0de-36bde2a630ba",
   "metadata": {},
   "source": [
    "There are no duplicate transactions in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf63bc7-5119-4459-9406-e675c94e50fc",
   "metadata": {},
   "source": [
    "#### 2.2.2: Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ddcc2-9cbe-4505-b92d-65a63f8e9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.isnull().sum() #confirm for any null entries in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b7b33-c70c-480c-acf4-356d068b6971",
   "metadata": {},
   "source": [
    "the column *'Postal Code'* is the only column with missing data. Luckily data such as Postal Code can easily be retrieved and inserted into the dataframe. Had the missing data been in other columns such as *'Sales'*, *'Quantity'*, *'Discount'* etc, we might be forced to evaluate how to deal with the missing values - whether to drop them all or whether to calculate a value for them based on other available values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61022d26-1d09-45b4-b027-afd1a1019c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df[store_df['Postal Code'].isnull()] #to find the specific missing Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0c462-c14d-47b2-8dcf-7426d5f3ed7a",
   "metadata": {},
   "source": [
    "All the 11 missing Postal Codes are for 'Burlington' City in Vermont state. We can easily search for that, and inserted into our dataframe using the .fillna() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6978c-830a-4902-847a-d023acf3c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Postal Code'] = store_df['Postal Code'].fillna(5401) #5401 is the Postal Code for Burlington City\n",
    "store_df.isnull().sum() #check to see if null values are still present in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b0f09-1652-4d18-89b9-08ce02f42646",
   "metadata": {},
   "source": [
    "### 2.2.3: Sorting Data Based On Order Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e69fc-5eb3-4d4a-bb58-388f47b99b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Order Date'] = pd.to_datetime(store_df['Order Date']) #changing datatype format to datetime instead of an object\n",
    "store_df.sort_values(by=['Order Date'], ascending=True, inplace=True)\n",
    "store_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b81c718-32d1-4509-8d8e-78f6c16ccb8f",
   "metadata": {},
   "source": [
    "#### 2.2.4: What Do They Sell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5718881-a263-46c3-af7c-16d50a20deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store_df['Category'].unique()) #check the categories of products sold by the store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40c805-bc7b-471e-a4b6-4047e40b0b24",
   "metadata": {},
   "source": [
    "The Store's products fall into these 3 Categories; with the following sub-categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80505843-287b-4777-b472-13db4e805346",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store_df['Sub-Category'].unique()) #sub-categories of products sold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32619815-4fc6-4a94-b0ab-0a90288df846",
   "metadata": {},
   "source": [
    "Later we will dive into analysis these interms of quantities and revenue generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ead4a-3001-45db-80cf-299e64a520ed",
   "metadata": {},
   "source": [
    "#### 2.2.5: Who Do They Sell To?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca27db1-6ed2-424a-aed2-f7e28649406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store_df['Segment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a68cbb-7d92-4093-83ce-6db138368996",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Segment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5832a3-5090-4a1f-9a7b-97b223a0b340",
   "metadata": {},
   "source": [
    "#### 2.2.6: Where Do They Sell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9d633-17f1-4cc1-81d7-f4620f13cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['State'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0cdde-9b9b-4521-b182-5b41153b23a6",
   "metadata": {},
   "source": [
    "Let's visualize these States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c886974-0e53-4c25-8efd-c7fd285e409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5)) #size of our plot\n",
    "plt.title('SuperStore - States Sold To Over The Review Period: 2015 - 2019', fontsize=12, fontweight='bold') #plot title\n",
    "plt.xticks(rotation='vertical') #orientation of the State names on the x-axis\n",
    "sns.countplot(x=store_df['State']) #what to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c9f75-20af-4eda-9416-2e48074bf3d1",
   "metadata": {},
   "source": [
    "With this we can already see which states seem to have more order coming from them and which have less. For example, it is visible that most of products have been sold to the States of California and New York; while less sales have gone to Wyoming and West Virginia States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b93b9-b9bb-482e-8d0b-5ef6f62fa814",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Region'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935393d-bad7-4059-8a4d-3294ad0e7039",
   "metadata": {},
   "source": [
    "The Store sales its products in 49 states that fall in 4 Regions. Later we will analysis the sales for each state and performance of each of the 4 regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c639471-6639-4504-9c56-bba2f0c4c1f1",
   "metadata": {},
   "source": [
    "## 3.0: Sales Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb778f0-4c08-404d-927d-8b8fd751c5a4",
   "metadata": {},
   "source": [
    "In this section we will divide our tasks inorder to query and visualize the following:\n",
    "\n",
    "-  Sales Trends Over Specified Duractions (Years and Months)\n",
    "-  The Store's Top Customers\n",
    "-  Average days taken for Order fullfilment i.e, from Order Date to Ship Date. Is it dependent on Shipping Mode?\n",
    "-  Product Performances\n",
    "-  Sales Based on Cities, Regions, Categories, Sub-Category (City with highest and lowest sales)\n",
    "-  Losses (Profit < 0)\n",
    "-  Correlations between Columns e.g., Discount and Profit among others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452876c-fbb3-42ec-8f3c-853fa3b7c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.columns #refresh our mind on our dataset's columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade01c1-d1e5-4bcf-a4eb-2f208f6e8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1bb3f-06a1-4aa1-bdb7-3c51208c42e6",
   "metadata": {},
   "source": [
    "### 3.1: What Duration does our Dataset Fall in?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61725e64-d3de-42c9-a9e8-dcd8423ed791",
   "metadata": {},
   "source": [
    "In order to get a better analysis of our date we will need some additional columns:\n",
    "-  Order Year\n",
    "-  Ship Year\n",
    "-  Order Month\n",
    "-  Ship Month\n",
    "-  Day Of Week the Order or Shipping happened\n",
    "\n",
    "Later when querying Order Fullfilment we will also need a column to capture the number of days it takes to fullfil an order (Difference between 'Order Date' and 'Ship Date')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f4970-eead-42a5-a5f3-42cd90641f70",
   "metadata": {},
   "source": [
    "#### 3.1.1: Getting the Years from the Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd05ef6-d52c-469e-8a0f-193b638883a4",
   "metadata": {},
   "source": [
    "To achieve this we will use the dt.year method. You can also use the dt.strftime ('%Y') method for the same result.\n",
    "Checking back on our dataframe, the datatype for 'Ship Date' is object; we need to change that to datetime datatype as we earlier did with Order Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21398da-5578-49d0-98d9-9ba2e55dcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Ship Date'] = pd.to_datetime(store_df['Ship Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1caf1-036a-4418-ba01-c530bfbc552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.info() #lets check to see if the datatype for the selected columns changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab72c2-e10e-417f-a4e0-eb8614d684c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Order Year'] = store_df['Order Date'].dt.year #creates a new column with the value for the Year of the Order\n",
    "store_df['Shipping Year'] = store_df['Ship Date'].dt.year #creates a new column with the value for the Year of the Shipping\n",
    "store_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590416c-d48b-40a1-8213-5505117bf505",
   "metadata": {},
   "source": [
    "We successfully added 'Order Year' and 'Shipping Year' to our dataframe. \n",
    "This will allow us to later focus on specific years and/or analysis trends over the years.\n",
    "Lets see the years present in our dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36242794-fa0f-4914-a20c-c08036901964",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store_df['Shipping Year'].unique()) #using the .unique() method to see the years our dataset spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df06be-351d-4973-b61e-b2a06db39a3b",
   "metadata": {},
   "source": [
    "The SuperStore dataset being analysed falls in the years 2015 to 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25932958-932d-473d-9bf4-2c1549d9591d",
   "metadata": {},
   "source": [
    "Let's proceed to add the months of each order and shipment to our dataframe using the .dt.month_name() method. Alternatively you could use .dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f34ef0-d3f2-4c74-bfb8-affc13b49842",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Order Month'] = store_df['Order Date'].dt.month_name() #new column Order Month\n",
    "store_df['Shipping Month'] = store_df['Order Date'].dt.month_name() #new column Shipping Month\n",
    "store_df[['Sales','Order Month','Order Year']].to_csv(\"data/SalesMonths-Years.csv\") #lets save our new dataset to a csv file but only with the sales, months and year columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa228e-1a8d-4092-9460-cd56c0882ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b27c25-842a-4f3a-91e0-5dd767712e9e",
   "metadata": {},
   "source": [
    "#### 3.1.2: Sales Trends Over Specified Duractions (Years and Months)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f5e79-c978-410e-99d8-2bfd4e3845b8",
   "metadata": {},
   "source": [
    "The reason behind extracting the Years and Months for each sale is to enable the Store to take note of the sales trends and months when sales were higher and lower, and even attribute internal or external reasons for those. Maybe Sales are higher on festival months, maybe Sales were lower in a specific month of 2018 because of a local or international event that affected Customers' purchases. We can then analysis how the months perform, if there is any pattern and even do a time series analysis and forecast sales. \n",
    "\n",
    "Let's plot some pivot tables of Sales (Number and Total Revenue) for the Months and Year(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f5b53-4e89-41fb-9431-331e6ef30a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our pivot table with the total amount generated from Sales\n",
    "total_sales_table = pd.pivot_table(store_df, values='Sales', index='Order Year', columns='Order Month', aggfunc='sum', margins=True).round(2)\n",
    "total_sales_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f3731-21d0-45cf-9624-b859779229e6",
   "metadata": {},
   "source": [
    "the pivot table above clearly shows as the Sales/Revenue totals in every month for every year present in the dataset, which the *margins=True* parameter adding the 'All' column for Totals of every year. All rounded off to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ba312-6745-41d3-b9ef-cf59ed37c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the count paramter to get the number of sales done in each year. The totals in the 'All' column should be equal to the total rows in the dataframe - 9800\n",
    "no_of_sales_table = pd.pivot_table(store_df, values='Sales', index='Order Year', columns='Order Month', aggfunc='count', margins=True).round(2)\n",
    "no_of_sales_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0b2bf-6a26-4514-a62d-43263d1291bd",
   "metadata": {},
   "source": [
    "We are now able to see the number of Sales that have happened over the years. It is observable that more sells happen in November and less in February, which each year having an increase number of sales done. We can visualize by ploting a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e03645-b317-4843-b93a-8447b4623bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_sales_table2 = no_of_sales_table.drop(columns=('All')) #drop the column 'All' it will affect the value scale of the heatmap, since its a Sum column\n",
    "no_of_sales_table2 = no_of_sales_table2.drop(index=('All')) #drop the index 'All' it will affect the value scale of the heatmap for the same reason\n",
    "no_of_sales_table2 #our new sales dataframe for the heatmap visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3ea1d-5af1-4a24-9e71-91b2faf01f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5)) #size of plot\n",
    "plt.title('SuperStore - Number of Sales Each Month for the Review Period: 2015 - 2019', fontsize=11, fontweight='bold') #title\n",
    "sns.heatmap(no_of_sales_table2, cmap='Blues', annot=True, annot_kws={\"size\":11}, fmt=\"d\", cbar=False) #the plot. annotation true, colors for map are Blues, and color bar disabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2afb41-6a76-4a27-a635-a91f40545ab0",
   "metadata": {},
   "source": [
    "The lighter the shade of blue the less the Number of Sales that year for the corresponding month. We can then observe that September 2018 had the most Number of Sales in the review period; and February 2015 had the lowest Number of Sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731e5aa-24eb-446b-93de-a94eb2fad725",
   "metadata": {},
   "source": [
    "We can also confirm the yearly total Revenue by creating a pivot table for Sales and Years only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75af7e-14cf-4127-a1c1-ac54884d7cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_sales_table = pd.pivot_table(store_df, 'Sales', index='Order Year', aggfunc=['sum', 'mean']).round(2)\n",
    "year_sales_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33011937-120f-474c-b357-2c9bdf1bf414",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_year = store_df.groupby(['Order Year']).sum().sort_values(\"Sales\", ascending=False).head(10) #sort  based on Sales\n",
    "sales_year = sales_year[['Sales']].round(2) #round off Sales to the nearest 2 decimal points\n",
    "sales_year.reset_index(inplace=True) #set Customer ID as a column and create a new index for this 'top_ten_customers' dataframe\n",
    "sales_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd205a5-7c24-4d3b-857a-51475c959220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(sales_year['Order Year'], sales_year['Sales'], color='#ecb365') #what to plot\n",
    "plt.title('SuperStore - Total Revenue for the Review Period: 2015 - 2019', fontsize=9, fontweight='bold') #title of out plot\n",
    "plt.xlabel('Year') #our axis labels\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Total Revenue') \n",
    "#for i, j in sales_year['Sales'].items(): #our index, values and enumerator\n",
    "#    plt.text(i, j+3, '$'+str(j)); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756018f-6111-4429-80db-62fdbf658e7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2: Who Are The Top Customers of The Store?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63375d6-9f99-4b11-afc2-30259d5bb512",
   "metadata": {},
   "source": [
    "-  Their Top Buyers\n",
    "-  Revenues and Sales in Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadefc38-ab00-44d6-aba1-14af206a19bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "We had previously removed the 'Customer Name' column from our dataframe so for this query we will use the 'Customer ID' column to find out who are the top customers of the store based off of the Sales. Maybe the store can throw in rewards for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a1f97-2a4a-4641-a7b3-ee38d0f0e998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_ten_customers = store_df.groupby(['Customer ID']).sum().sort_values(\"Sales\", ascending=False).head(10) #sort Customers based on Sales\n",
    "top_ten_customers = top_ten_customers[['Sales']].round(2) #round off Sales to the nearest 2 decimal points\n",
    "top_ten_customers.reset_index(inplace=True) #set Customer ID as a column and create a new index for this 'top_ten_customers' dataframe\n",
    "top_ten_customers #view the top 10 customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97593d-144c-4941-bde5-bfbbc54de707",
   "metadata": {},
   "source": [
    "plot for this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cdff2-807c-4660-b5de-15abb577fc64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5)) #size of the plot\n",
    "plt.bar(top_ten_customers['Customer ID'], top_ten_customers['Sales'], color='#99f5e0', edgecolor='green') #what to plot\n",
    "plt.title('SuperStore - Top Ten Customers Over the Review Period: 2015 - 2019', fontsize=11, fontweight='bold') #title of out plot\n",
    "plt.xlabel('Customer ID') #our axis labels\n",
    "plt.ylabel('Total Spendings') \n",
    "for i, j in top_ten_customers['Sales'].items(): #our index, values and enumerator\n",
    "    plt.text(i, j-10000, '$'+str(j), fontsize=11, rotation='vertical', horizontalalignment='center'); #annotation of Sales values in each bar and specifying the position of the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d14cb-79d3-4cd7-8fc7-bcbeaabd502e",
   "metadata": {},
   "source": [
    "### 3.2.1: Revenue and Sales in terms of the Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061d376",
   "metadata": {},
   "source": [
    "Let's see Revenues generated by the three Segments for the review period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181e4db-91ed-4586-8584-84e5702efefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_sales = store_df.groupby(['Segment', 'Order Year'])[[\"Sales\"]].sum().round(2).reset_index() #round off revenues and reset index\n",
    "segment_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "sns.countplot(x=store_df.Segment)\n",
    "plt.title('SuperStore - Sales and Segments', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.xlabel('Segment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36f7b4-9e01-4f28-964f-16e9c0f18c64",
   "metadata": {},
   "source": [
    "### 3.3: The Store's Order fullfilment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f159639-4bde-4ec0-98f2-501bfb91ffdd",
   "metadata": {},
   "source": [
    "-  The Shipping Modes\n",
    "-  Average days taken for Order fullfilment i.e, from Order Date to Ship Date\n",
    "-  Does Fullfilment duration depend on Shipping Mode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6263481e-3f50-4629-af6b-ab04c88f690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Ship Mode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547cd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "store_df['Ship Mode'].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035dddb-5366-471e-94b5-53b5f84c8c2f",
   "metadata": {},
   "source": [
    "the Standard Class shipping mode is the most opted for by customers for their orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7867b4-d08c-498c-96b4-d00b788c79b3",
   "metadata": {},
   "source": [
    "Let's obtain the Order fullfilment days i.e., the days it takes from Order Date to the Shipping Date...how long the store takes to fullfil an order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27b955-1e04-4c3b-bd7f-73e3238faf1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store_df['Fullfilment Days'] = (store_df['Ship Date'] - store_df['Order Date']).abs() #a new column that gets its values from the difference of the two columns\n",
    "#added abs() cause the initial gave negative values in some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a757c91-1c5f-4feb-ae0a-731207764a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.sample(2) #check if column added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c518f9-9657-42f2-a078-8f600113fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Fullfilment Days'] = store_df['Fullfilment Days'].astype(str) #changing Fullfilment Days column values to strings to allow removal of 'Days'\n",
    "store_df.dtypes\n",
    "store_df['FullfilmentDays'] = store_df['Fullfilment Days'].apply(lambda x:x.split (\" \")[0]) #create a new column with only the first part digit of the Fullfilment Days column\n",
    "store_df.drop(columns='Fullfilment Days', inplace=True) #then drop the old column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca500daa-e285-406b-9e26-dda66bfde485",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['FullfilmentDays'] = store_df['FullfilmentDays'].astype(int) #turn the new column into integer data type for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a2529-abec-46a8-aa45-6bc9cfeedd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['FullfilmentDays'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049bb44-24d9-43f2-993f-a406902e1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['FullfilmentDays'].sort_values().median().round(0) #the median avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b51e1-d45e-4228-b1c0-d1842ba593c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(store_df, 'FullfilmentDays', index='Ship Mode', aggfunc='median').round(0).reset_index() #avg days taken to fullfil orders based on shipping mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de8f04-cb33-4bb8-99da-e174a69d3e09",
   "metadata": {},
   "source": [
    "Standard Class shipping option, on average, takes way longer than the other 3 options. Despite this it is the most opted for, as earlier seen, with 5859 orders out of the 9800. So Customers do not seem to care much for how fast they get their order shipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e40b32-a9a0-406c-bc87-1a33c2ad4374",
   "metadata": {},
   "source": [
    "Another way of getting the same results but sorted from largest and plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9450b-47fb-43d3-a57a-1145eb7baf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.groupby(by='Ship Mode').mean()['FullfilmentDays'].nlargest().round(0).plot.bar(ylabel='Fullfilment Days', xlabel='Shipping Mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a4fc2-5b9f-4f96-94d5-152d341d8f39",
   "metadata": {},
   "source": [
    "### 3.4: Product Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e45530-5d2c-4c6c-af8a-e7bf4fd0233c",
   "metadata": {},
   "source": [
    "-  Performance of Categories and Sub-Categories\n",
    "-  Top Selling Products\n",
    "-  Units Sold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd0f35",
   "metadata": {},
   "source": [
    "#### 3.4.1: Categories Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9e319-1548-4194-82a1-2c9a1ea0597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_category = store_df.groupby([\"Category\"]).sum().sort_values(\"Sales\", ascending=False)  #Sort the Categories as per the sales\n",
    "top_category = top_category[[\"Sales\"]] #keep only the sales column in the dataframe\n",
    "total_revenue_category = top_category[\"Sales\"].sum() #the total revenue generated as per category\n",
    "total_revenue_category = str(int(total_revenue_category)) #Convert the total_revenue_category from float to int and then to string\n",
    "total_revenue_category = '$' + total_revenue_category #Adding '$' sign before the Value\n",
    "top_category.reset_index(inplace=True) #Since we have used groupby, we will have to reset the index to add the category into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d122568",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (13,5) #size of plot\n",
    "plt.rcParams['font.size'] = 12 #font size\n",
    "plt.rcParams['font.weight'] = 6 #font weight\n",
    "# we don't want to look at the percentage distribution in the pie chart. Instead, we want to look at the exact revenue generated by the categories.\n",
    "def autopct_format(values): \n",
    "    def my_format(pct): \n",
    "        total = sum(values) \n",
    "        val = int(round(pct*total/100.0))\n",
    "        return ' ${v:d}'.format(v=val)\n",
    "    return my_format\n",
    "colors = ['#00ffed','#6454f0','#ee4d5f'] #colors the pie chart\n",
    "explode = (0.05,0.05,0.05)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(top_category['Sales'], colors = colors, labels=top_category['Category'], autopct= autopct_format(top_category['Sales']), startangle=90,explode=explode)\n",
    "centre_circle = plt.Circle((0,0),0.82,fc='white') #drawing a circle on the pie chart to make it look better \n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle) #add the circle on the pie chart\n",
    "#equal aspect ratio ensures that pie is drawn as a circle\n",
    "ax1.axis('equal') \n",
    "#the total revenue generated by all the categories at the center\n",
    "label = ax1.annotate('Total Revenue \\n'+str(total_revenue_category),color = 'red', xy=(0, 0), fontsize=12, horizontalalignment=\"center\")\n",
    "plt.title('SuperStore - Contribution of Each Category to Total Revenue: 2015 - 2019') #title of plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577775f2",
   "metadata": {},
   "source": [
    "#### 3.4.2: Product Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee5de78",
   "metadata": {},
   "source": [
    "##### 3.4.2.1: Products with highest Revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c8000-6a00-4106-8be9-70bbb1df8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products=store_df.groupby(['Product Name']).sum().sort_values('Sales',ascending=False).head(10) #get Sales for all products, sort highest first then display only the first 10\n",
    "top_products=top_products[['Sales']].round(2) #round off Sales to the nearest 2 decimal points\n",
    "top_products.reset_index(inplace=True)\n",
    "top_products #show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c60f23-826f-40e5-9bf1-858cb82cfde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5)) #size of plot\n",
    "plt.bar(top_products['Product Name'],top_products['Sales'],color='#d57eea',edgecolor='green') #what to plot\n",
    "plt.xticks(rotation='vertical') #xaxis ticks orientation\n",
    "plt.title('SuperStore - Ten Products with The Highest Revenues for The review period: 2015 - 2019',fontsize=12, fontweight='bold') #title of the plot\n",
    "plt.xlabel('Product',fontsize=12) #xaxis labels\n",
    "plt.ylabel('Total Revenue',fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255d37c",
   "metadata": {},
   "source": [
    "##### 3.4.2.2: Units Sold for Each Product, and by Category and Sub-Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Quantity'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b3595",
   "metadata": {},
   "source": [
    "the Store sold a total of 37143 products during the period of 2015 - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362f8fb-2d75-40e8-a548-da59b50617c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_sold = store_df.groupby(['Product Name'])[[\"Quantity\"]].sum().sort_values('Quantity', ascending=False).reset_index().head(15) #get quantities sold by product and sort by highest and limit to 15\n",
    "units_sold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7a5f4",
   "metadata": {},
   "source": [
    "As observed, staples, staple envelops and easy-staple paper were three of the most sold products by the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5)) #size of plot\n",
    "plt.bar(units_sold['Product Name'],units_sold['Quantity'],color='#d57eea',edgecolor='green') #what to plot\n",
    "plt.xticks(rotation='vertical') #xaxis ticks orientation\n",
    "plt.title('SuperStore - Most Purchased Products in the Review Period: 2015 - 2019',fontsize=12, fontweight='bold') #title of the plot\n",
    "plt.xlabel('Product',fontsize=12) #xaxis labels\n",
    "plt.ylabel('Units Sold',fontsize=12)\n",
    "for i, j in units_sold['Quantity'].items(): #our index, values and enumerator\n",
    "    if j>200:\n",
    "        plt.text(i, j-50, str(j), fontsize=11, rotation='vertical', horizontalalignment='center');\n",
    "    else:\n",
    "        plt.text(i, j+4, str(j), fontsize=11, rotation='vertical', horizontalalignment='center');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea9fd3d-f1bd-4a19-8576-54519e036a71",
   "metadata": {},
   "source": [
    "### 3.5: Sales Based on Cities and Regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cfaa97",
   "metadata": {},
   "source": [
    "#### 3.5.1: Regions and Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8993736",
   "metadata": {},
   "source": [
    "Regional distribution of the 9800 orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1d510-753a-43b5-817b-323ac7f8cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['Region'].value_counts() #show regions and the number of sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7773fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4)) #size of plot\n",
    "plt.title('SuperStore - Sales Made in Regions: 2015 - 2019', fontsize=11, fontweight='bold') #title of plot\n",
    "plt.xlabel('Region') #xaxis label\n",
    "plt.ylabel('Sales Made') #ylabel of plot\n",
    "sns.countplot(x=store_df['Region']) #what to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f8d89",
   "metadata": {},
   "source": [
    "#### 3.5.2: Sales and Revenue based on States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f54061",
   "metadata": {},
   "source": [
    "##### 3.5.2.1: States with Highest Revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_states = store_df.groupby([\"State\"]).sum().sort_values(\"Sales\", ascending=False).head(20) # Sort the States as per the sales\n",
    "top_states = top_states[[\"Sales\"]].round(2) # Round off the Sales Value up to 2 decimal places\n",
    "top_states.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(top_states['State'],top_states['Sales'],color='#9fa5d5',edgecolor='blue')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title('SuperStore - 20 Cities with The Highest Revenues for The review period: 2015 - 2019',fontsize=12, fontweight='bold')\n",
    "plt.xlabel('State',fontsize=12)\n",
    "plt.ylabel('Total Revenue',fontsize=12)\n",
    "for i,j in top_states[\"Sales\"].items(): #To show the exact revenue generated on the figure\n",
    "    if j>400000:\n",
    "        plt.text(i,j-150000,'$'+ str(j), fontsize=12,rotation=90,color='k', horizontalalignment='center'); #annotations inside chart if its above 400000\n",
    "    else:\n",
    "        plt.text(i,j+15000,'$'+ str(j), fontsize=12,rotation=90,color='k', horizontalalignment='center'); #else annotations above bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7cae2",
   "metadata": {},
   "source": [
    "As observed, the State of California generated the most revenue for SuperStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8760b3",
   "metadata": {},
   "source": [
    "##### 3.5.2.2: States with Lowest Sales and Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d62d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.groupby(['State']).sum()['Sales'].nsmallest(10) #10 states that generated the lowest revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5e44d",
   "metadata": {},
   "source": [
    "### 3.7: Losses Experienced By SuperStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90aab2",
   "metadata": {},
   "source": [
    "Despite an overall growth in profit, the Store also had its share of losses. To analyze this we will take the Profit column and filter out a dataframe with Profit less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ef19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_df = store_df[store_df['Profit'] < 0] #a new data frame with the records that have 'Profit'less than 0\n",
    "losses_df.shape #see affected records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7710f0d",
   "metadata": {},
   "source": [
    "Of the 9800 transactions, 1847 resulted in losses for the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a435ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = np.negative(losses_df['Profit'].sum().round(2)) #sum of negative values\n",
    "\n",
    "print(\"and the total loss for the Review Period (2015-2019) is %2f\"%total_loss) #print the total loss in a statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd423d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(losses_df['Sub-Category'], losses_df['Sales'])\n",
    "plt.title('SuperStore - Losses in Sub-Categories: 2015 - 2019', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.xlabel('Sub-Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b49b4",
   "metadata": {},
   "source": [
    "### 3.8: Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d8261",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df_correlations = store_df.corr(method='pearson') #we will use the pearson correlation matrix\n",
    "sns.heatmap(store_df_correlations, annot=True) #our plot with annotations of results\n",
    "plt.title('SuperStore - Correlation Matrix of Column Elements', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Sales Features')\n",
    "plt.ylabel('Sales Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79d5dd",
   "metadata": {},
   "source": [
    "Straight away from the plot, we notice that Discount and Profit have a negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=store_df['Discount'], y=store_df['Profit'], alpha=0.5)\n",
    "plt.title('SuperStore - Discount vs Profit', fontsize=11, fontweight='bold')\n",
    "plt.xlabel('Discount')\n",
    "plt.ylabel('Profit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0be38b",
   "metadata": {},
   "source": [
    "#### 3.8.1: Numerizing All Columns So They Can Be Included In The Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a6d91",
   "metadata": {},
   "source": [
    "As seen above the columns that do not contain numeric values are ignored. To include them, we have to numerize the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da836ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df_numeric = store_df.drop(columns='Country').apply(lambda x: x.factorize()[0]).corr(method='pearson')\n",
    "store_df_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633deb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "sns.heatmap(store_df_numeric, annot=True) #our plot with annotations of results\n",
    "plt.title('SuperStore - Correlation Matrix of Column Elements', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Sales Features')\n",
    "plt.ylabel('Sales Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54923d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_pairs = store_df_numeric.unstack()\n",
    "correlated_pairs.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_corr = correlated_pairs[(correlated_pairs) > 0.5]\n",
    "higher_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f58d1-f548-4b9c-bd0a-823602c37825",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba3ed9ec-4376-4b82-8e6e-46fef415afc6",
   "metadata": {},
   "source": [
    "[Part 2: Customer Retention](https://github.com/WayneNyariroh/customer-retention_cohortAnalysis/blob/main/RetentionAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759fe2e-7f35-4fcc-b694-9c0496e1cc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "f3d5247997b242e2ee84c3d630eeeccac3e67b2682a45443aeb85a33b102e51d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
